{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See flickr_crawler.py . \n",
    "There are many available APIs for websites like Flickr, Google, Bing, Twitter that allows to crawl through content and download available images based on queries.\n",
    "For this experiment, we want to recognize fruits in photos. Some samples of the data retrieved: \n",
    "    \n",
    "<table><tr><td><img src='images/train/cherry/img0000.jpg' width=80></td><td><img src='images/train/cherry/img0003.jpg' width=80></td><td><img src='images/train/banana/img0000.jpg' width=80></td><td><img src='images/train/banana/img00011.jpg' width=80></td><td><img src='images/train/pineapple/img0000.jpg' width=80></td><td><img src='images/train/pineapple/img0007.jpg' width=80></td><td><img src='images/train/lemon/img0000.jpg' width=80></td><td><img src='images/train/lemon/img0002.jpg' width=80></td><td><img src='images/train/lychee/img0000.jpg' width=80></td><td><img src='images/train/lychee/img0001.jpg' width=80></td><td><img src='images/train/blackberry/img0001.jpg' width=80></td><td><img src='images/train/blackberry/img0004.jpg' width=80></td></tr></table>\n",
    "\n",
    "It's not perfect but it works ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format and load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PyTorch functionalities for this. With PyTorch, two modules you should know about for data handling:\n",
    " - Dataset : where and how to access your data. Is your data images, arrays, is it labelled, shoud it be transformed ...\n",
    " - Dataloader : load the dataset. Can transfer it directly to the GPU for faster training execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_transforms = {\n",
    "            'train': transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                         transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "            'val': transforms.Compose([transforms.Resize((224,224)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "        }\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder('images/train', data_transforms['train'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "val_dataset = datasets.ImageFolder('images/test', data_transforms['val'])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A small example with the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we will work with PyTorch classes. PyTorch allows you to build your own Modules (aka networks), your own Losses, while inheriting the features of Modules and Losses classes.\n",
    "We can build a multilayer perceptron to see how it's done :\n",
    "\n",
    "<img src=\"https://www.oreilly.com/library/view/getting-started-with/9781786468574/graphics/B05474_04_05.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class myPerceptron(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_hidden_1, nb_hidden_2, nb_outputs):\n",
    "        super().__init__()\n",
    "        self.add_module(\"linear1\", nn.Linear(nb_inputs, nb_hidden_1))\n",
    "        self.add_module(\"activation1\", nn.ReLU())\n",
    "        self.add_module(\"linear2\", nn.Linear(nb_hidden_1, nb_hidden_2))\n",
    "        self.add_module(\"activation2\", nn.Tanh())\n",
    "        self.add_module(\"output\", nn.Linear(nb_hidden_2, nb_outputs))\n",
    "        self.add_module(\"softmax\", nn.Softmax(dim=0))\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for module in self.children():\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "    \n",
    "myperceptron = myPerceptron(4, 16, 8, 2)\n",
    "input = torch.randn(4)\n",
    "myperceptron.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice : create your perceptron and train it with the sonar data (sonar_data.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about deep convolutional neural networks ...\n",
    "\n",
    "PyTorch.torchvision come with several already implemented famous CNN architectures. These models are also pretrained on ImageNet, which can significally speed up the learning process. \n",
    "Let's see what is inside a VGG for example:\n",
    "\n",
    "<img src=\"pres/vgg_net.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.vgg11()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our deep network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head over to the models.py file and find the DeepNet class. It includes:\n",
    " - a basemodel (resnet, vgg, or alexnet) pretrained on ImageNet for feature extraction\n",
    " - the modified last layer for finetuning a basemodel on our data\n",
    " - functions to forward, predict, and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DeepNet\n",
    "\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "NB_CLASSES = 16\n",
    "NUM_EPOCHS = 16\n",
    "\n",
    "# 1 Create the deep net\n",
    "deep_net = DeepNet('resnet', NB_CLASSES, full_backprop=False)\n",
    "# 2 Select a loss. CrossEntropyLoss is recommended for multiclass problems.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# 3 Select an optimizer. This will update the weights in the network during the backpropagation\n",
    "# You can try with Adam, RMSProp, SGD with different learning rates and parameters...\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,deep_net.parameters()))\n",
    "# 4 Train the deep_net\n",
    "if CUDA_AVAILABLE:\n",
    "    deep_net.fit(train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are several examples of how the training went and performed for several optimizers, optimizer parameters, and architectures. You can plot your own results with `plt.plot(deep_net.tr_accuracy)` and `plt.plot(deep_net.val_accuracy)`\n",
    "\n",
    "        Training and validation accuracies over epochs for Alexnet, VGG, Resnet as basemodels.\n",
    "<table><tr><td><img src='plots/deep_net_alexnet.png', width='400'></td><td><img src='plots/deep_net_vgg.png', width=400></td><td><img src='plots/deep_net_adam.png', width=400></td></tr></table>\n",
    "\n",
    "                    Different learning rates with batch size, stochastic gradient steps and momentum\n",
    "<table><tr><td><img src='plots/deep_net_fc_sbs.png', width='400'></td><td><img src='plots/deep_net_fc_sgd0001.png', width=400></td><td><img src='plots/deep_net_fc_sgd0001m.png', width=400></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to avoid training your own model, or if you don't have a GPU, you can load a pre-trained model by executing the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DeepNet\n",
    "\n",
    "deep_net = DeepNet('resnet', 16, full_backprop=False)\n",
    "if CUDA_AVAILABLE:\n",
    "    deep_net.load_state_dict(torch.load('models/deep_net_fc_adam.pt'))\n",
    "else:\n",
    "    deep_net.load_state_dict(torch.load('models/deep_net_fc_adam.pt', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network\n",
    "\n",
    "We will now test in inference with new and different images, from Bing and some photos I took myself. You could test with your own pictures of fruits by adding the .jpg in the test_images folder.\n",
    "The inference process is as follows:\n",
    " - retrieve the images in the local folder, and compute the path (for opening and visualisation purposes)\n",
    " - open the images (with PIL library)\n",
    " - transforms the images (now in bytes) into tensors. This is done with the data_transforms we introduced above\n",
    " \n",
    " - set the network in 'eval' mode. This avoids unnecessary gradient computations, since we are not trying to 'learn' anything at this stage\n",
    " - use the predict() function on the inferences images. The result is a prediction and its probability for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "images = [files for root, dirs, files in os.walk('test_images/')][0]\n",
    "images_paths = ['test_images/'+im for im in images]\n",
    "images_objects = [Image.open(im) for im in images_paths]\n",
    "tensors = torch.stack([data_transforms['val'](im) for im in images_objects])\n",
    "\n",
    "deep_net.eval()\n",
    "probabilities, predictions = deep_net.predict(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image, display\n",
    "def display_image_prediction(image_path, prediction, probability):\n",
    "    imageQuery = \"<img style='margin: 1px; width:200px; float: left; border: 1px solid ;' src={} '/>\".format(\n",
    "        image_path)\n",
    "    display(HTML(imageQuery))\n",
    "    print('Prediction: {}, {}\\n'.format(prediction, probability))\n",
    "    \n",
    "for i in range(len(predictions)):\n",
    "    prediction = train_dataset.classes[predictions[i].item()]\n",
    "    probability = round(100*probabilities[i].item(), 1)\n",
    "    display_image_prediction(images_paths[i], prediction, probability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
